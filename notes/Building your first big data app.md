# Building your first big data app 

## Technologies mentioned:

* [AWS Kinesis](https://aws.amazon.com/kinesis) - AWS  Kinese is an easy to setup managed service to handle events streams.
There are three offerings: 
  * [AWS data streams](https://aws.amazon.com/kinesis/data-streams) -  for handling ingoing and outgoing events.
  * [AWS data analytics](https://aws.amazon.com/kinesis/data-analytics) - To apply data analytics and data handling. You can either use [Apache Flink](https://flink.apache.org/) 
  or   sql. The video showcased how to apply unsupervised learning algorithm random forest and setup pumbs  
  * [AWS data Firehose](https://aws.amazon.com/kinesis/data-firehose) - Managed service to move events to AWS data stores( AWS S3, AWS redshift ..etc)
  
* [AWS lambda](https://aws.amazon.com/lambda/) - Can be used to offer data transformation beyond what's availabel in SQL think (Calling external APIs for enriching the events).
The lambda functions acts as puller consuming events as soon as it's available in the stream.
 
* [AWS glue data cataloge](https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html): AWS glue is an ETL framework for running and manting ETL jobs. 
One feature that's offered by AWS glue is the data cataloge which is an Aapache HIVE compatible metastore. Meaning, that it's used to track the definitions of tables and where are their data resides. 
In the talk the transformed events where stored and partitioned using AWS kineses a AWS glue crawler was used to infer the schema. 
Which then was made available AWS athena for querying
  
* [AWS Athena](https://aws.amazon.com/athena/) - Managed offering from [Facebook presto](https://prestodb.github.io/) A distributed data engine that decouple data storage from the SQL engine.
In case of AWS the data is stored on S3. The schema in AWS glue and the querying happens on Athena.

## Notes: 
 
 In AWS data analytics in AWS kinesis there's two important concepts which are [pumbs and streams](https://docs.aws.amazon.com/kinesisanalytics/latest/dev/streams-pumps.html)
 
 ### Streams:
 Steams are a continous stream of data that is generated by applying a pump
 
 ### Pumbs: 
 Pumbs are basically a `SELECT` query that is continously run to populate the stream.
